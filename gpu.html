<!DOCTYPE html>
<html>
<body>
<canvas id="gpu-canvas"></canvas>
<video id="video" autoplay muted playsinline style="display:none"></video>
<script type="module">
async function init(){
  const video = document.getElementById('video');
  const stream = await navigator.mediaDevices.getUserMedia({video:true});
  video.srcObject = stream;
  await new Promise(r => video.addEventListener('loadedmetadata', r));
  video.play();

  const canvas = document.getElementById('gpu-canvas');
  canvas.width = video.videoWidth;
  canvas.height = video.videoHeight;
  const adapter = await navigator.gpu.requestAdapter();
  const device = await adapter.requestDevice();
  const context = canvas.getContext('webgpu');
  const format = 'bgra8unorm';
  context.configure({ device, format, alphaMode: 'opaque' });

  const sampler = device.createSampler({ magFilter: 'linear', minFilter: 'linear' });
  const videoTexture = device.createTexture({
    size: [canvas.width, canvas.height],
    format,
    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.RENDER_ATTACHMENT
  });

  const quadModule = device.createShaderModule({ code: `
@vertex fn vs(@builtin(vertex_index) i: u32)->@builtin(position) vec4<f32>{
  var pos = array<vec2<f32>,4>(
    vec2(-1,-1), vec2(1,-1),
    vec2(-1,1),  vec2(1,1));
  return vec4(pos[i],0,1);
}
@group(0) @binding(0) var videoTex: texture_2d<f32>;
@group(0) @binding(1) var videoSampler: sampler;
@fragment fn fs(@builtin(position) coord: vec4<f32>)->@location(0) vec4<f32>{
  let uv = coord.xy / vec2(${canvas.width}.0, ${canvas.height}.0);
  return textureSample(videoTex, videoSampler, uv);
}` });
  const quadPipeline = device.createRenderPipeline({
    layout: 'auto',
    vertex: { module: quadModule, entryPoint: 'vs' },
    fragment: { module: quadModule, entryPoint: 'fs', targets: [{ format }] },
    primitive: { topology: 'triangle-strip' }
  });
  const quadBindGroup = device.createBindGroup({
    layout: quadPipeline.getBindGroupLayout(0),
    entries: [
      { binding: 0, resource: videoTexture.createView() },
      { binding: 1, resource: sampler }
    ]
  });

  const lineModule = device.createShaderModule({ code: `
@vertex fn vs2(@location(0) pos: vec2<f32>)->@builtin(position) vec4<f32>{
  return vec4(pos,0,1);
}
@fragment fn fs2()->@location(0) vec4<f32>{
  return vec4(1,0,0,1);
}` });
  const linePipeline = device.createRenderPipeline({
    layout: 'auto',
    vertex: {
      module: lineModule,
      entryPoint: 'vs2',
      buffers: [{ arrayStride: 8, attributes: [{ shaderLocation: 0, offset: 0, format: 'float32x2' }] }]
    },
    fragment: { module: lineModule, entryPoint: 'fs2', targets: [{ format }] },
    primitive: { topology: 'line-list' }
  });

  function frame(){
    device.queue.copyExternalImageToTexture(
      { source: video }, { texture: videoTexture }, [canvas.width, canvas.height]
    );
    const encoder = device.createCommandEncoder();
    const pass = encoder.beginRenderPass({
      colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: 'clear', storeOp: 'store' }]
    });
    pass.setPipeline(quadPipeline);
    pass.setBindGroup(0, quadBindGroup);
    pass.draw(4);
    const lines = 10;
    const verts = new Float32Array(lines*4);
    for(let i=0;i<lines*2;i++){
      verts[i*2]   = Math.random()*2-1;
      verts[i*2+1] = Math.random()*2-1;
    }
    const vb = device.createBuffer({
      size: verts.byteLength,
      usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST
    });
    device.queue.writeBuffer(vb, 0, verts);
    pass.setPipeline(linePipeline);
    pass.setVertexBuffer(0, vb);
    pass.draw(lines*2);
    pass.end();
    device.queue.submit([encoder.finish()]);
    requestAnimationFrame(frame);
  }
  requestAnimationFrame(frame);
}
init();
</script>
</body>
</html>
